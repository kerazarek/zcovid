{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covid stuff\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "- **created** by z: `2020-03-30`\n",
    "- last **updated**: `2020-04-02T13:39:50PDT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _preamble_\n",
    "\n",
    "#### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import requests\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display as disp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### disable request warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cwd = pathlib.Path.cwd()\n",
    "# cwd\n",
    "# docs_dir = pathlib.Path(\"/private/var/mobile/Library/Mobile Documents/iCloud~AsheKube~Carnets/Documents\")\n",
    "# data_dir = docs_dir / \"data\"\n",
    "\n",
    "# params\n",
    "on_ipad = False # false if on mac\n",
    "read_from_csse = True # read from the CSSE dir, not zcovid data dir\n",
    "overwrite_csvs = False # (only if not CSSE) overwrite CSVs if already existing\n",
    "\n",
    "# basic paths\n",
    "HOME = pathlib.Path.home()\n",
    "zcovid_root = HOME / \"Dropbox/code/github/zcovid\"\n",
    "csse_data_dir = zcovid_root / \"COVID-19/csse_covid_19_data\"\n",
    "csse_tsdata_dir = csse_data_dir / \"csse_covid_19_time_series\"\n",
    "icloud_root = pathlib.Path(\"/private/var/mobile/Library/Mobile Documents\")\n",
    "carnets_dir = icloud_root / \"iCloud~AsheKube~Carnets/Documents\"\n",
    "\n",
    "# where data will be saved/loaded\n",
    "# data_dir = pathlib.Path(\"/Users/zarek/Dropbox/code/github/zcovid/data\")\n",
    "# data_dir = pathlib.Path(\"/Users/zarek/Dropbox/code/github/zcovid/COVID-19/csse_covid_19_data/csse_covid_19_time_series\")\n",
    "\n",
    "if on_ipad:\n",
    "    data_dir = carnets_dir / \"data\" \n",
    "elif read_from_csse:\n",
    "    data_dir = csse_tsdata_dir\n",
    "    overwrite_csvs = False\n",
    "else:\n",
    "    data_dir = zcovid_root / \"data\"\n",
    "    \n",
    "if not data_dir.is_dir():\n",
    "    data_dir.mkdir()\n",
    "    print(\">>> created dir {}\".format(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base URL for data downloads\n",
    "base_tsdata_url = \"https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_re = re.compile(r\"\\d+/\\d+/\\d+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time-series keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file names and saving data\n",
    "tskeys = [\n",
    "    'confirmed_US',\n",
    "    'confirmed_global',\n",
    "    'deaths_US',\n",
    "    'deaths_global',\n",
    "    'recovered_global'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinum(): format number SI\n",
    "\n",
    "SINUM_PREFIXES = {\n",
    "    -6: {'short': \"a\", 'long': \"atto\"},\n",
    "    -5: {'short': \"f\", 'long': \"femto\"},\n",
    "    -4: {'short': \"p\", 'long': \"pico\"},\n",
    "    -3: {'short': \"n\", 'long': \"nano\"},\n",
    "    -2: {'short': \"Î¼\", 'long': \"micro\"},\n",
    "    -1: {'short': \"m\", 'long': \"milli\"},\n",
    "    0: {'short': \" \", 'long': \"-\"},\n",
    "    1: {'short': \"k\", 'long': \"kilo\"},\n",
    "    2: {'short': \"M\", 'long': \"mega\"},\n",
    "    3: {'short': \"G\", 'long': \"giga\"},\n",
    "    4: {'short': \"T\", 'long': \"tera\"},\n",
    "    5: {'short': \"P\", 'long': \"peta\"},\n",
    "    6: {'short': \"E\", 'long': \"exa\"}\n",
    "}\n",
    "\n",
    "def sinum(num, unit='B', fmt=\"{coef:.3f} {pfx}{unit}\", long_pfx=False, strip_zeros=True, binary=False, verbose=False):\n",
    "\n",
    "    # check inputs\n",
    "    assert isinstance(num, (int, float))\n",
    "    assert isinstance(unit, str)\n",
    "    \n",
    "    # verbosity...\n",
    "    if verbose:\n",
    "        print(f\">>> sinum(num={num!r}, unit={unit!r}, fmt={fmt!r}, long_pfx={long_pfx}, strip_zeros={strip_zeros}, binary={binary}, verbose={verbose})\")\n",
    "    def _verb(name, value):\n",
    "        if verbose:\n",
    "            print(\"\\t{:<12s} = {:>20s}\".format(\n",
    "                name, \n",
    "                value if isinstance(value, str) else repr(value)\n",
    "            ))\n",
    "            \n",
    "    # binary mods\n",
    "    if binary:\n",
    "        log_base = 1024\n",
    "        unit = f\"i{unit!s}\"        \n",
    "    else:\n",
    "        log_base = 1000\n",
    "        unit = str(unit)\n",
    "        \n",
    "    # order of magnitude\n",
    "    if num == 0:\n",
    "        oom = 0\n",
    "    else:\n",
    "        oom = math.floor(math.log(-num if num < 0 else num, log_base))\n",
    "    if oom < -6:\n",
    "        oom = -6\n",
    "    if oom > 6:\n",
    "        oom = 6\n",
    "    _verb('oom', oom)\n",
    "    \n",
    "    # coefficient\n",
    "    coef = num / (log_base ** oom)\n",
    "    _verb('coef', coef)\n",
    "    \n",
    "    # SI prefix\n",
    "    pfx = SINUM_PREFIXES[oom]['long' if long_pfx else 'short']\n",
    "    _verb('pfx', pfx)\n",
    "    \n",
    "    # string out\n",
    "    out = fmt.format(coef=coef, pfx=pfx, unit=unit)\n",
    "    # if strip_zeros:\n",
    "    #     while re.match(r\"\\d+\\.0{2,}\", out):\n",
    "    #         # out = re.sub(r\"(?:\\d+(?:\\.0*)?)0\", ' ', out)\n",
    "    #         out = re.sub(r\"(\\d+\\.0*?)(0)\\b\", '\\1 ', out)\n",
    "    _verb('out', out)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# tests\n",
    "if False:\n",
    "    print(sinum(82457891234, verbose=True), end='\\n\\n')\n",
    "    print(sinum(82457891234, verbose=True, unit=''), end='\\n\\n')\n",
    "    print(sinum(82457891234, verbose=True, unit='bloops'), end='\\n\\n')\n",
    "    print(sinum(82457891234, binary=True, verbose=True), end='\\n\\n')\n",
    "    print(sinum(0.00082457891234, verbose=True), end='\\n\\n')\n",
    "    print(sinum(824578912342345.2345, verbose=True), end='\\n\\n')\n",
    "    print(sinum(0.00082457891234, binary=True, verbose=True), end='\\n\\n')\n",
    "    print(sinum(824578912342345.2345, binary=True, verbose=True), end='\\n\\n')\n",
    "    print(sinum(824578912342345.2345, binary=True, verbose=True, fmt=\"{coef:12.5f} // {pfx} // {unit}\"), end='\\n\\n')\n",
    "    print(sinum(824578912342345.2345, binary=True, fmt=\"{coef:12.5f} // {pfx} // {unit}\"), end='\\n\\n')\n",
    "    print(sinum(1, verbose=True), end='\\n\\n')\n",
    "    print(sinum(-1, verbose=True), end='\\n\\n')\n",
    "    print(sinum(0, verbose=True), end='\\n\\n')\n",
    "    print(sinum(-82457891234, verbose=True), end='\\n\\n')\n",
    "    print(sinum(8786996786798967896872457891234, verbose=True), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n",
    "\n",
    "#### initialize data container `d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confirmed_US': {},\n",
       " 'confirmed_global': {},\n",
       " 'deaths_US': {},\n",
       " 'deaths_global': {},\n",
       " 'recovered_global': {}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a dict with tskeys as keys to uniform dicts\n",
    "d = {}\n",
    "for tsk in tskeys:\n",
    "    d[tsk] = {}\n",
    "disp(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download data from github, save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> loading CSVs, saving in ``/Users/zarek/Dropbox/code/github/zcovid/COVID-19/csse_covid_19_data/csse_covid_19_time_series''\n",
      "\n",
      ">>> getting data for 'confirmed_US'\n",
      "--> CSV for 'confirmed_US' already exists and won't be overwritten, skipping\n",
      ">>> getting data for 'confirmed_global'\n",
      "--> CSV for 'confirmed_global' already exists and won't be overwritten, skipping\n",
      ">>> getting data for 'deaths_US'\n",
      "--> CSV for 'deaths_US' already exists and won't be overwritten, skipping\n",
      ">>> getting data for 'deaths_global'\n",
      "--> CSV for 'deaths_global' already exists and won't be overwritten, skipping\n",
      ">>> getting data for 'recovered_global'\n",
      "--> CSV for 'recovered_global' already exists and won't be overwritten, skipping\n"
     ]
    }
   ],
   "source": [
    "# same load and save process for each tskey\n",
    "print(f\">>> loading CSVs, saving in ``{data_dir}''\\n\")\n",
    "for tsk in tskeys:\n",
    "    \n",
    "    print(\">>> getting data for '{}'\".format(tsk))\n",
    "    \n",
    "    # download URL from github\n",
    "    d[tsk]['url'] = f\"{base_tsdata_url}/csse_covid_19_time_series/time_series_covid19_{tsk}.csv\"\n",
    "    \n",
    "    # CSV to load from and/or write to\n",
    "    d[tsk]['csv'] = data_dir / f\"time_series_covid19_{tsk}.csv\"\n",
    "    \n",
    "    # check for existing file\n",
    "    if d[tsk]['csv'].is_file() and d[tsk]['csv'].stat().st_size > 0:\n",
    "        if overwrite_csvs:\n",
    "            print(f\"--> CSV for '{tsk}' already exists and will be overwritten\")\n",
    "        else:\n",
    "            print(f\"--> CSV for '{tsk}' already exists and won't be overwritten, skipping\")    \n",
    "            continue # skip to next tskey\n",
    "            \n",
    "    # web request\n",
    "    d[tsk]['req'] = requests.get(d[tsk]['url'], auth=('user', 'pass'))\n",
    "    \n",
    "    # CSV contents as a big string\n",
    "    d[tsk]['raw'] = d[tsk]['req'].content.decode() # decode cause it opens as bytes\n",
    "    \n",
    "    # save the CSV\n",
    "    with d[tsk]['csv'].open('w') as f:\n",
    "        print(\"--> writing ``.../{}'' ... \".format(d[tsk]['csv'].name), end='')\n",
    "        f.write(d[tsk]['raw'])\n",
    "        print(\"wrote {}\\n\".format(sinum(d[tsk]['csv'].stat().st_size)))\n",
    "        \n",
    "    # ditch that big string\n",
    "    del d[tsk]['raw']\n",
    "    \n",
    "# disp(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data from CSV just saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate tskeys, loading from CSV saved above\n",
    "for tsk in tskeys:\n",
    "    d[tsk]['df'] = pd.read_csv(d[tsk]['csv'])\n",
    "    print(f\"--> read CSV data for {tsk}\")\n",
    "    \n",
    "# create backup of d\n",
    "d_BAK = d.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean up data\n",
    "\n",
    "#### add index columns `d[tsk]['df']` (actual dataframe for the key), then reorder as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column name substitutions\n",
    "col_subs = {\n",
    "    'Province_State': 'subregion',\n",
    "    'Province/State': 'subregion',\n",
    "    'Country_Region': 'region',\n",
    "    'Country/Region': 'region',\n",
    "    'Long_': 'long'    \n",
    "} \n",
    "\n",
    "# columns to move to the beginning (in order)\n",
    "priority_cols = [\n",
    "    'locid',\n",
    "    'region',\n",
    "    'subregion',\n",
    "    'combined_key',\n",
    "    'lat',\n",
    "    'long',\n",
    "    'population'\n",
    "]\n",
    "\n",
    "# collect all columns since different dataframes dont have same columns\n",
    "all_indx_cols = []\n",
    "all_date_cols = []\n",
    "\n",
    "# iterate through tskeys, cleaning up each\n",
    "for tsk in tskeys:\n",
    "    \n",
    "    print(f\">>> cleaning up dataframe for '{tsk}'\")\n",
    "    \n",
    "    # add other index cols\n",
    "    d[tsk]['df']['tskey'] = tsk\n",
    "    d[tsk]['df']['domain'] = tsk.split('_')[1]\n",
    "    d[tsk]['df']['datum'] = tsk.split('_')[0]\n",
    "    d[tsk]['df']['locid'] = d[tsk]['df'].index\n",
    "    \n",
    "    d[tsk]['all_cols'] = list(d[tsk]['df'].columns)\n",
    "    \n",
    "    # clean up column names\n",
    "    for i, c in enumerate(d[tsk]['all_cols']):\n",
    "        if c in col_subs:\n",
    "            c = col_subs[c]\n",
    "        c = c.lower()\n",
    "        d[tsk]['all_cols'][i] = c\n",
    "    # print(d[tsk]['all_cols'])\n",
    "    \n",
    "    # get column subsets\n",
    "    d[tsk]['df'].columns = d[tsk]['all_cols']\n",
    "    d[tsk]['date_cols'] = list(filter(date_re.match, d[tsk]['all_cols']))\n",
    "    d[tsk]['indx_cols'] = [i for i in d[tsk]['all_cols'] if i not in d[tsk]['date_cols']]\n",
    "\n",
    "    # reorder columns\n",
    "    col_idxs = list(range(len(d[tsk]['indx_cols'])))\n",
    "    for col in priority_cols[::-1]:\n",
    "        if col in d[tsk]['indx_cols']:\n",
    "            idx = d[tsk]['indx_cols'].index(col)\n",
    "            col_idxs.remove(idx)\n",
    "            col_idxs.insert(0, idx)\n",
    "    print(col_idxs)\n",
    "    d[tsk]['indx_cols'] = [d[tsk]['indx_cols'][i] for i in col_idxs]\n",
    "        \n",
    "    # add to all_indx_cols\n",
    "    for col in d[tsk]['indx_cols']:\n",
    "        if col not in all_indx_cols:\n",
    "            all_indx_cols.append(col)\n",
    "\n",
    "    # add to all_date_cols\n",
    "    for col in d[tsk]['date_cols']:\n",
    "        if col not in all_date_cols:\n",
    "            all_date_cols.append(col)\n",
    "\n",
    "    # save dataframe with reordered columns\n",
    "    d[tsk]['all_cols'] = [*d[tsk]['indx_cols'], *d[tsk]['date_cols']]\n",
    "    d[tsk]['df'] = d[tsk]['df'][d[tsk]['all_cols']]\n",
    "\n",
    "# d[tsk]\n",
    "\n",
    "print(all_indx_cols)\n",
    "print(all_date_cols)\n",
    "\n",
    "disp(d[tsk]['df'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create backups of `d[tsk]['df']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tsk in tskeys:\n",
    "    # print(d[tsk]['indx_cols'])\n",
    "    # if type(d[tsk]['df'].columns).__name__ == 'Index':\n",
    "    if ('df_BAK' not in d[tsk]) or isinstance(d[tsk]['df'].columns, pd.Index):\n",
    "        d[tsk]['df_BAK'] = d[tsk]['df'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert `d[tsk]['df']` such that rows are dates and columns are multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tsk in tskeys:\n",
    "    \n",
    "    # create multiindex dataframe (df with just index cols)\n",
    "    mindx_df = d[tsk]['df_BAK'][d[tsk]['indx_cols']]\n",
    "    # create multiindex\n",
    "    mindx = pd.MultiIndex.from_frame(mindx_df)\n",
    "    # create new dataframe, old df transposed\n",
    "    d[tsk]['df'] = d[tsk]['df_BAK'][d[tsk]['date_cols']].transpose()\n",
    "    # add the new multiindex\n",
    "    d[tsk]['df'].columns = mindx\n",
    "    # convert index from str to datetime\n",
    "    d[tsk]['df'].index = pd.to_datetime(d[tsk]['df'].index)\n",
    "    \n",
    "# mindx_df\n",
    "# mindx\n",
    "\n",
    "# disp(d[tskeys[0]]['df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add levels to multi-index of `d[tsk]['df']` so they are uniform across all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tsk in tskeys:  # [tskeys[1]]\n",
    "    \n",
    "    # # re-collect multiindex from dataframe\n",
    "    # mindx = d[tsk]['df'].columns\n",
    "    # # re-create mindx df\n",
    "    # mindx_df = mindx.to_frame()\n",
    "    # # reindex to reorder columns and add empty keys for any missing\n",
    "    # mindx_df = mindx_df.reindex(columns=all_indx_cols)                \n",
    "    # # # below is alternative way to do reindex\n",
    "    # # for col in all_indx_cols:\n",
    "    # #     if col not in mindx_df.columns:\n",
    "    # #         mindx_df = mindx_df.assign(**{col: np.nan})\n",
    "    # # change that dataframe back into a multiindex\n",
    "    # mindx = pd.MultiIndex.from_frame(mindx_df)\n",
    "    # # assign the multiindex back into the original data\n",
    "    # d[tsk]['df'].columns = mindx\n",
    "\n",
    "    # below does all the above as a one-liner (expanded for readability)\n",
    "    d[tsk]['df'].columns = pd.MultiIndex.from_frame(\n",
    "        d[tsk]['df'].columns\n",
    "            .to_frame()\n",
    "            .reindex(columns=all_indx_cols)\n",
    "    )\n",
    "\n",
    "# print(all_indx_cols)\n",
    "# print(mindx_df.columns)\n",
    "# mindx_df\n",
    "\n",
    "# disp(d[tskeys[3]]['df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile data\n",
    "\n",
    "#### create joined dataframe from individual `d[tsk]['df']`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize new key in d, with first df as its df\n",
    "d['all'] = {'df': d[tskeys[0]]['df']}\n",
    "\n",
    "# iterate through rest of dfs, joining into 'all' df\n",
    "for tsk in tskeys[1:]:\n",
    "    d['all']['df'] = d['all']['df'].join(d[tsk]['df'])\n",
    "    \n",
    "# print(d['all']['df'].shape)\n",
    "disp(d['all']['df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute secondary data\n",
    "\n",
    "#### columns to be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcs = {\n",
    "#     'deaths_per_confirmed': lambda df:\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### actually compute them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.shape)\n",
    "sdf = df.iloc[:, (glv('region') == \"United Kingdom\") & (glv('domain') == \"global\")]\n",
    "print(sdf.shape)\n",
    "\n",
    "# sdf\n",
    "# sdf.columns.to_frame()\n",
    "disp(sdf)\n",
    "disp(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot stuff!\n",
    "\n",
    "#### ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = d['all']['df']\n",
    "glv = df.columns.get_level_values\n",
    "\n",
    "# df\n",
    "print(list(glv('region').unique().sort_values()))\n",
    "# df.iloc[:, glv('region') == \"US\"]\n",
    "# df.iloc[:, (glv('region') == \"US\") | (glv('region') == \"United Kingdom\")]\n",
    "# df.iloc[:, glv('region') in [\"US\", \"United Kingdom\"]]\n",
    "# df.columns\n",
    "# df.columns.to_frame().loc[:, 'region'].unique()\n",
    "# glv(1)\n",
    "# glv('region') == \"US\"\n",
    "# usdf = df.iloc[:, (glv('region') == \"US\") & np.isnan(glv('subregion'))]\n",
    "# usdf\n",
    "df.iloc[:, (glv('region') == \"US\") & (glv('subregion').isna())]\n",
    "# list(usdf.columns.to_frame().subregion.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data\n",
    "df = d['all']['df']\n",
    "glv = df.columns.get_level_values\n",
    "\n",
    "# plotting params\n",
    "prms = {\n",
    "    'domain': [\"global\"],\n",
    "    'datum': [\"confirmed\"],\n",
    "    'region': [\"US\", \"United Kingdom\", \"China\"],\n",
    "    'subregion': [np.nan]\n",
    "}\n",
    "\n",
    "# subset data\n",
    "# sdf = df.iloc[:, (glv('domain') == 'global') & (glv('datum') == 'confirmed')].iloc[:, :3]\n",
    "sbool = np.ones(df.shape[1], bool)\n",
    "# print(sum(sbool))\n",
    "for prm, vals in prms.items():\n",
    "    sbool = sbool & (glv(prm).isin(vals))\n",
    "    # print(sbool.shape)\n",
    "sdf = df.iloc[:, sbool]\n",
    "sglv = sdf.columns.get_level_values\n",
    "psdf = sdf.copy()\n",
    "psdf.columns = list(sglv('region'))\n",
    "\n",
    "disp(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "fignum = 0\n",
    "fig = plt.figure(num=fignum, figsize=(15, 10), dpi=80)\n",
    "\n",
    "# plt.plot(sdf)\n",
    "# plt.legend(sglv('region'))\n",
    "print(psdf.columns)\n",
    "plt.plot(psdf)\n",
    "plt.legend(list(psdf.columns))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Scatter(\n",
    "        x=psdf.index,\n",
    "        y=psdf.US\n",
    "    ),\n",
    "])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig, axs = plt.subplots(2,1, figsize = (10,6))\n",
    "axs[0].plot(psdf)\n",
    "# fig\n",
    "# ax.plot(psdf.US)\n",
    "plotly_fig = tls.mpl_to_plotly(fig) ## convert \n",
    "iplot(plotly_fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, glv('region') == \"US\"] #.columns.to_frame().subregion.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fignum = 0\n",
    "fig = plt.figure(num=fignum, figsize=(30, 50), dpi=80)\n",
    "\n",
    "df = d['all']['df']\n",
    "glv = df.columns.get_level_values\n",
    "# df = df.iloc[:, (glv('region') == \"US\") & (glv('subregion').isna() | (glv('subregion') == \"California\"))]\n",
    "df = df.iloc[:, (glv('region') == \"US\") & (glv('subregion').isna() & (glv('subregion') == \"California\"))]\n",
    "glv = df.columns.get_level_values\n",
    "\n",
    "# print(list(glv('combined_key').unique()))\n",
    "# print([x for x in (glv('combined_key').isna())])\n",
    "# print(list(glv('combined_key')[(~glv('combined_key').isna()) & (glv('combined_key').to_series().str.contains(\"California\"))]))\n",
    "# print(list(glv('combined_key')[(glv('combined_key').isna()) | (glv('combined_key').to_series().str.contains(\"California\"))]))\n",
    "# plot_regions = plot_data.columns[[0, 1, 2]].get_level_values('region')\n",
    "# plt.plot(df)\n",
    "# plt.legend(plot_regions)\n",
    "\n",
    "df\n",
    "\n",
    "# plt.plot(df)\n",
    "# plt.legend(glv('combined_key'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
